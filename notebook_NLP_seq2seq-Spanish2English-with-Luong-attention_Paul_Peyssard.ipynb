{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78508b33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-22T13:37:04.063861Z",
     "start_time": "2022-09-22T13:37:04.055329Z"
    }
   },
   "source": [
    "# Sequence to Sequence (seq2seq) variable length and Attention\n",
    "\n",
    "The most popular sequence-to-sequence task is translation: usually, from one natural language to another. In the last couple of years, commercial systems became surprisingly good at machine translation - check out, for example, Google Translate, Yandex Translate, DeepL Translator, Bing Microsoft Translator.\n",
    "\n",
    "More information on the lecture and [here](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html) (basics+attention part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0de8cd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T17:17:34.909908Z",
     "start_time": "2023-01-26T17:17:33.831458Z"
    }
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71870716",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T17:17:37.366411Z",
     "start_time": "2023-01-26T17:17:36.131204Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df7ae156",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T17:17:42.537097Z",
     "start_time": "2023-01-26T17:17:37.370117Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae82589a",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The data is available here: http://www.manythings.org/anki/. They consist of a set of pairs in the following format:\n",
    "\n",
    "  `May I borrow this book?`  `¿Puedo tomar prestado este libro?`\n",
    "\n",
    "There are a variety of languages available, English-Spanish data will be used.\n",
    "\n",
    "The processing of the data will consist of the following steps:\n",
    "\n",
    "1. Removal of special characters\n",
    "1. adding a start or end token to each sentence.\n",
    "1. Creating the vectorizer for each of the two languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3480543e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T17:18:26.593257Z",
     "start_time": "2023-01-26T17:18:26.418041Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data download\n",
    "def download_nmt(corpus=\"spa-eng\"):\n",
    "    path_to_zip = tf.keras.utils.get_file(corpus+\".zip\",\n",
    "                                          origin=\"http://storage.googleapis.com/download.tensorflow.org/data/\"+corpus+\".zip\",\n",
    "                                          extract=True)\n",
    "    path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\"\n",
    "    return path_to_file\n",
    "\n",
    "file_path = download_nmt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aafee2bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T17:18:31.264620Z",
     "start_time": "2023-01-26T17:18:31.257606Z"
    }
   },
   "outputs": [],
   "source": [
    "# Process the data\n",
    "def step1(sent):\n",
    "    # sent = on sentence in a language\n",
    "    def unicode_to_ascii(s):\n",
    "        return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "    sent = unicode_to_ascii(sent.lower().strip())\n",
    "\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    sent = re.sub(r\"([?.!,¿])\", r\" \\1 \", sent)\n",
    "    sent = re.sub(r'[\" \"]+', \" \", sent)\n",
    "\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    sent = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sent)\n",
    "\n",
    "    return '<start> ' + sent.strip() + ' <end>' # Suppress extra space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14709205",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T17:18:33.295792Z",
     "start_time": "2023-01-26T17:18:33.095822Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<start> salga de aqui ! <end>', '<start> go away ! <end>')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data\n",
    "def read_data(path, num_examples, split=0.2):\n",
    "    # path : path to spa-eng.txt file\n",
    "    # num_examples : Limit the total number of training example for faster training (set num_examples = len(lines) to use full data)\n",
    "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "    english_sentences, spanish_sentences= zip(*[[step1(sent) for sent in l.split('\\t')]  for l in lines[:int((1+split)*num_examples)]])\n",
    "\n",
    "    return np.array(spanish_sentences), np.array(english_sentences)\n",
    "\n",
    "num_examples = 700 # of which 100 for testing\n",
    "\n",
    "spanish_sentences, english_sentences = read_data(file_path, num_examples, split=0.0)\n",
    "test_spanish_sentences = spanish_sentences[num_examples-100:]\n",
    "test_english_sentences = english_sentences[num_examples-100:]\n",
    "spanish_sentences = spanish_sentences[:num_examples-100]\n",
    "english_sentences = english_sentences[:num_examples-100]\n",
    "spanish_sentences[100], english_sentences[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6710f03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T17:18:34.523968Z",
     "start_time": "2023-01-26T17:18:34.514163Z"
    }
   },
   "outputs": [],
   "source": [
    "# Search vocabulary and max_length for each language\n",
    "def voc(lang):\n",
    "    # a list of sentences in the same language\n",
    "    lengths = [len(txt.split()) for txt in lang]\n",
    "    vocab = set([w for txt in lang for w in txt.split()])  \n",
    "\n",
    "    return max(lengths), list(vocab), len(vocab)+2 # for padding and OOV\n",
    "\n",
    "max_length_spanish, vocab_spanish, vocab_size_spanish = voc(spanish_sentences)\n",
    "max_length_english, vocab_english, vocab_size_english = voc(english_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6507d38c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T17:18:35.805497Z",
     "start_time": "2023-01-26T17:18:35.787503Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>max_length</th>\n",
       "      <th>vocab_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>english</td>\n",
       "      <td>7</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spanish</td>\n",
       "      <td>8</td>\n",
       "      <td>548</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      lang  max_length  vocab_size\n",
       "0  english           7         279\n",
       "1  spanish           8         548"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\"lang\":[\"english\", \"spanish\"],\n",
    "              \"max_length\":[max_length_english, max_length_spanish],\n",
    "              \"vocab_size\": [vocab_size_english, vocab_size_spanish]}).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cd5098a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T17:18:41.848113Z",
     "start_time": "2023-01-26T17:18:41.790093Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build vectorizer layer\n",
    "english_vectorizer = layers.TextVectorization(standardize=None, output_mode='int',\n",
    "                                              vocabulary=vocab_english,\n",
    "                                              name=\"English_vect\")\n",
    "# On peut connaitre le vocabulaire par english_vectorizer.get_vocabulary()\n",
    "\n",
    "# Do the same for spanish\n",
    "spanish_vectorizer = layers.TextVectorization(standardize=None, output_mode='int',\n",
    "                                              vocabulary=vocab_spanish,\n",
    "                                              name=\"Spanish_vect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f94d9977",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T17:18:42.920562Z",
     "start_time": "2023-01-26T17:18:42.916171Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "embedding_dim = 50\n",
    "latent_dim = 64\n",
    "dropout=0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d57e539",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-25T15:45:09.728517Z",
     "start_time": "2022-09-25T15:45:09.728489Z"
    }
   },
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de7dfe84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T17:19:02.901943Z",
     "start_time": "2023-01-26T17:19:02.891732Z"
    }
   },
   "outputs": [],
   "source": [
    "attention_func = 'MLP' # others possibility: 'dot', 'bilinear', 'MLP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cac81860",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T17:19:08.446139Z",
     "start_time": "2023-01-26T17:19:08.074180Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, None, 64)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Encoder part \"\"\"\n",
    "encoder_inputs = layers.Input(shape=(1,), dtype=tf.string, name=\"spanish\")\n",
    "encoder_vectorizer = spanish_vectorizer(encoder_inputs) #transform sentence in tokens (output=length)\n",
    "encoder_embedding = layers.Embedding(vocab_size_spanish, embedding_dim, #embedd each token\n",
    "                                     name=\"spanish_embedding\")(encoder_vectorizer)\n",
    "encoder_outputs, encoder_state_h, encoder_state_c = layers.LSTM(\n",
    "    latent_dim, \n",
    "    return_sequences=True,\n",
    "    return_state=True,\n",
    "                           \n",
    "    dropout=dropout, \n",
    "    recurrent_dropout=dropout, \n",
    "    name=\"encoder\")(encoder_embedding)\n",
    "\n",
    "# We discard `encoder_outputs` and only keep the states. \n",
    "encoder_states = [encoder_state_h, encoder_state_c]\n",
    "\n",
    "print(encoder_outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a966059",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T17:19:14.740437Z",
     "start_time": "2023-01-26T17:19:14.463892Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, None, 64)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Decoder part \"\"\"\n",
    "# Set up the decoder, using `encoder_states` as initial state. \n",
    "decoder_inputs = layers.Input(shape=(None,), dtype=tf.int32, name=\"english_teacher\")\n",
    "decoder_embedding = layers.Embedding(vocab_size_english, embedding_dim,\n",
    "                                     name=\"english_embedding\")(decoder_inputs)\n",
    "decoder_outputs, decoder_state_h, decoder_state_c = layers.LSTM(latent_dim, return_sequences=True, return_state=True,\n",
    "                                                                dropout=dropout, recurrent_dropout=dropout, name=\"Decoder\")(decoder_embedding, initial_state=encoder_states)\n",
    "\n",
    "print(decoder_outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc14c6a",
   "metadata": {},
   "source": [
    " ![General computation scheme](https://lena-voita.github.io/resources/lectures/seq2seq/attention/computation_scheme-min.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f58816ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T17:19:38.718475Z",
     "start_time": "2023-01-26T17:19:38.515519Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP\n",
      "(None, None, None)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Attentional part \"\"\"\n",
    "# We use all encoder ouput\n",
    "\n",
    "if attention_func=='dot':\n",
    "    # Scoring funtion Dot\n",
    "    print(\"dot\")\n",
    "    attention = layers.dot([decoder_outputs, encoder_outputs], axes=[2,2], name=\"Attention_score\")\n",
    "elif attention_func=='bilinear':\n",
    "    # Scoring function Bilinear\n",
    "    print(\"bilinear\")\n",
    "    attention = layers.Dense(latent_dim, activation=\"linear\", name=\"dense_bilinear\")(encoder_outputs)\n",
    "    attention = layers.dot([decoder_outputs, attention], axes=[2,2], name=\"Attention_score\")\n",
    "elif attention_func==\"MLP\": #Here we use this one cuz we stick to MLP\n",
    "    # Scoring function MLP - need to know encoder_max_len / decoder_max_len\n",
    "    print(\"MLP\")\n",
    "    attention_encoder = layers.Dense(latent_dim, activation=\"tanh\", name=\"dense_encoder\")(encoder_outputs)\n",
    "    attention_decoder = layers.Dense(latent_dim, activation=\"tanh\", name=\"dense_decoder\")(decoder_outputs)\n",
    "    attention = layers.dot([attention_decoder, attention_encoder], axes=[2,2], name=\"Attention_score\")\n",
    "\n",
    "if attention_func is not None:\n",
    "    attention = layers.Activation('softmax', name=\"Attention_weight\")(attention)\n",
    "    print(attention.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d0ba539",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T17:19:40.095690Z",
     "start_time": "2023-01-26T17:19:40.078648Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, None, 64)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Context vector \"\"\"\n",
    "if attention_func is not None:\n",
    "    context = layers.dot([attention, encoder_outputs], axes=[2,1], name=\"Attention_output\")\n",
    "    print(context.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d88fc633",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T17:19:41.471114Z",
     "start_time": "2023-01-26T17:19:41.450748Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with attention\n",
      "(None, None, 128)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Combine attention with decoder ouput \"\"\"\n",
    "if attention_func is None:\n",
    "    print(\"without attention\")\n",
    "    decoder_combined_context = decoder_outputs\n",
    "else:\n",
    "    print(\"with attention\")\n",
    "    decoder_combined_context = layers.concatenate([context, decoder_outputs], name=\"Attention\")\n",
    "print(decoder_combined_context.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24e883d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T17:19:44.697040Z",
     "start_time": "2023-01-26T17:19:44.646692Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, None, 64)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Ouput of the model \"\"\"\n",
    "ouputs = layers.Dense(vocab_size_english, activation='softmax', name=\"Output\")(decoder_combined_context)\n",
    "print(decoder_outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d884e03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T17:37:02.910369Z",
     "start_time": "2023-01-26T17:37:02.712209Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39msummary()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd638f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T17:36:54.002991Z",
     "start_time": "2023-01-26T17:36:53.282603Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" The model \"\"\"\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], ouputs)\n",
    "\n",
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6597ef08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T17:19:58.437467Z",
     "start_time": "2023-01-26T17:19:58.385678Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare training data with teacher forcing\n",
    "# decoder_input_data is a 2D array of shape (None, None) containing a tokenization of the English sentences.\n",
    "# decoder_target_data is the same as decoder_input_data but offset by one timestep. decoder_target_data[:, t] will be the same as decoder_input_data[:, t-1].\n",
    "\n",
    "english_teacher_enc = english_vectorizer(english_sentences).numpy() # Teacher\n",
    "english_target_enc = np.zeros_like(english_teacher_enc)\n",
    "english_target_enc[:,:-1] = english_teacher_enc[:,1:] # To predict\n",
    "english_target_enc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6ff9c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T17:20:03.216269Z",
     "start_time": "2023-01-26T17:20:03.187188Z"
    }
   },
   "outputs": [],
   "source": [
    "#model.compile(loss='sparse_categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a99095",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-27T20:56:40.588508Z",
     "start_time": "2022-09-27T20:56:40.576904Z"
    }
   },
   "source": [
    "<font color=red>\n",
    "We voluntarily choose to stop on loss in order to have overfitting: too little data to train (or too long training time if we take more data).\n",
    "\n",
    "Obviously in real life... this is not something to do.\n",
    "\n",
    "Nevertheless, we keep a validation set to see the overtraining\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9f6887",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T17:23:02.027567Z",
     "start_time": "2023-01-26T17:20:20.197516Z"
    }
   },
   "outputs": [],
   "source": [
    "# Configure the model and start training\n",
    "# Stop training with early stopping with patience of 20\n",
    "callbacks_list = [EarlyStopping(monitor='loss', mode='min', min_delta=0.000001,\n",
    "                                patience=5, restore_best_weights=True, verbose=1,\n",
    "                                )]\n",
    "history = model.fit([spanish_sentences, english_teacher_enc], english_target_enc,\n",
    "                    validation_split=0.1,\n",
    "                    epochs=1000, batch_size=BATCH_SIZE,\n",
    "                    callbacks=callbacks_list,\n",
    "                    verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f750fa7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T17:23:21.827809Z",
     "start_time": "2023-01-26T17:23:02.033696Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save('seq2seq-attention-variable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59feece5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T17:23:22.328438Z",
     "start_time": "2023-01-26T17:23:21.830971Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Babysit the model\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(18, 5))\n",
    "\n",
    "pd.DataFrame({'val_loss':history.history['val_loss'],\n",
    "              'loss':history.history['loss'],\n",
    "             }).plot(ax=ax1)\n",
    "pd.DataFrame({'val_accuracy':history.history['val_accuracy'],\n",
    "              'accuracy':history.history['accuracy']\n",
    "             }).plot(ax=ax2)\n",
    "\n",
    "min(history.history['loss']), max(history.history['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3697386d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T17:27:16.110444Z",
     "start_time": "2023-01-26T17:27:16.095778Z"
    }
   },
   "outputs": [],
   "source": [
    "attention_func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1145ea97",
   "metadata": {},
   "source": [
    "* no attention : (0.170416459441185, 0.9534391760826111)\n",
    "* dot : (0.19616758823394775, 0.9447090029716492)\n",
    "* bilinear : (0.266628623008728, 0.9269841313362122)\n",
    "* MLP : (0.12090284377336502, 0.9666666388511658)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff3d499",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fe5f28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T17:27:21.705705Z",
     "start_time": "2023-01-26T17:27:21.689971Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pour décoder, il faut transformer un entier en mot.\n",
    "token2word = {token:word for token, word in enumerate(english_vectorizer.get_vocabulary())}\n",
    "word2token = {v:k for k, v in token2word.items()}\n",
    "\n",
    "def decode_english_sentence(lang):\n",
    "    return \" \".join([token2word[t] for t in lang]).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526513a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T17:27:22.291924Z",
     "start_time": "2023-01-26T17:27:22.281634Z"
    }
   },
   "outputs": [],
   "source": [
    "i2w = {token:word for token, word in enumerate(spanish_vectorizer.get_vocabulary())}\n",
    "#wi = {v:k for k, v in i2w.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2193c268",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T17:27:22.944857Z",
     "start_time": "2023-01-26T17:27:22.937345Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def translate(spanish_sent, preprocessing=False):\n",
    "    # Encode the input as state vectors.\n",
    "    if not preprocessing:\n",
    "        spanish_sent = step1(spanish_sent)\n",
    "    \n",
    "    # Sampling loop for decoding one sentence.\n",
    "    stop_condition = False\n",
    "    steps = 0\n",
    "    \n",
    "    # Build teacher for first step i.e. populate the first word of target sequence with \"<start>\".\n",
    "    target_seq = np.zeros(((1, max_length_english)))\n",
    "    target_seq[0, steps] = word2token[\"<start>\"]\n",
    "    \n",
    "    while not stop_condition:        \n",
    "        # Predict\n",
    "        predicted_tokens = model.predict([np.array(spanish_sent, ndmin=2), target_seq], verbose=0)\n",
    "        predicted_tokens = np.argmax(predicted_tokens, axis=2)\n",
    "        \n",
    "        # Sample a token\n",
    "        sampled_token_index = predicted_tokens[0, steps]\n",
    "        \n",
    "        # Update the target sequence.\n",
    "        steps += 1\n",
    "        target_seq[0, steps] = sampled_token_index\n",
    "        \n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if sampled_token_index==word2token['<end>'] or steps==max_length_english-1:\n",
    "            stop_condition = True\n",
    "    return decode_english_sentence(target_seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5403562f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T17:31:18.906320Z",
     "start_time": "2023-01-26T17:27:23.643401Z"
    }
   },
   "outputs": [],
   "source": [
    "# Permet de rechercher les phrases correctes.\n",
    "from tqdm import tqdm\n",
    "\n",
    "correct = []\n",
    "incorrect = []\n",
    "for i, (spanish_sent, english_sent) in tqdm(enumerate(zip(spanish_sentences, english_sentences))):\n",
    "    decoded_sentence = translate(spanish_sent)\n",
    "    decoded = english_vectorizer(decoded_sentence).numpy()\n",
    "    original = english_vectorizer(english_sent).numpy()\n",
    "    if len(decoded)==len(original) and sum(decoded==original)==len(original):\n",
    "        correct += [i]\n",
    "    else:\n",
    "        incorrect += [i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849a15f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T17:31:18.941526Z",
     "start_time": "2023-01-26T17:31:18.922432Z"
    }
   },
   "outputs": [],
   "source": [
    "len(correct), len(correct)/len(spanish_sentences)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127ef248",
   "metadata": {},
   "source": [
    "On train set:\n",
    "* with variable length (208, 34.66)\n",
    "* with MLP attention and variable length (218, 36.333333333333336)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e10f8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T17:31:23.275993Z",
     "start_time": "2023-01-26T17:31:18.951181Z"
    }
   },
   "outputs": [],
   "source": [
    "# Décode les phrases correctes sélectionnées\n",
    "for spanish_sent, english_sent in zip(spanish_sentences[correct[-10:]], english_sentences[correct[-10:]]):\n",
    "    print(\"=\"*50)\n",
    "    print(spanish_sent, \"-->\", english_sent)\n",
    "    decoded_sentence = translate(spanish_sent)\n",
    "    print(english_sent, \"???\", decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7614f834",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T17:31:27.762848Z",
     "start_time": "2023-01-26T17:31:23.282708Z"
    }
   },
   "outputs": [],
   "source": [
    "# Décode les phrases incorrectes sélectionnées\n",
    "for spanish_sent, english_sent in zip(spanish_sentences[incorrect[-10:]], english_sentences[incorrect[-10:]]):\n",
    "    print(\"=\"*50)\n",
    "    print(spanish_sent, \"-->\", english_sent)\n",
    "    decoded_sentence = translate(spanish_sent)\n",
    "    print(english_sent, \"???\", decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bd2168",
   "metadata": {},
   "source": [
    "##  Performance on never before seen data\n",
    "\n",
    "<font color='red'>\n",
    "    <bold>Warning:</bold> remember that we have overfitted the model and that we are only working on train data... and not test data.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e013021c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-26T17:32:10.210646Z",
     "start_time": "2023-01-26T17:31:27.766827Z"
    }
   },
   "outputs": [],
   "source": [
    "# Permet de rechercher les phrases correctes.\n",
    "from tqdm import tqdm\n",
    "\n",
    "test_correct = []\n",
    "for i, (spanish_sent, english_sent) in tqdm(enumerate(zip(test_spanish_sentences, test_english_sentences))):\n",
    "    decoded_sentence = translate(spanish_sent)\n",
    "    decoded = english_vectorizer(decoded_sentence).numpy()\n",
    "    original = english_vectorizer(english_sent).numpy()\n",
    "    if len(decoded)==len(original) and sum(decoded==original)==len(original):\n",
    "        test_correct += [i]\n",
    "len(test_correct), len(test_correct)/len(test_spanish_sentences)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad0431b",
   "metadata": {},
   "source": [
    "## Plotting attention matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2ad7eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-19T09:36:35.899853Z",
     "start_time": "2022-10-19T09:36:35.543632Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" The activation matrix is extracted from the previous model. \"\"\"\n",
    "attention_model = Model([encoder_inputs, decoder_inputs], attention)\n",
    "plot_model(attention_model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b3d87f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-19T09:37:29.229830Z",
     "start_time": "2022-10-19T09:37:29.227327Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Apply model on one sentence \"\"\"\n",
    "id = 74 # 13, 42, 74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4393a143",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-19T10:13:52.372353Z",
     "start_time": "2022-10-19T10:13:52.157800Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "attention_matrix = attention_model.predict([spanish_sentences[id:id+1], english_teacher_enc[id:id+1]])[0]\n",
    "\n",
    "x_labels = [spanish_vectorizer.get_vocabulary()[i] for i in spanish_vectorizer(spanish_sentences[id]).numpy() if i!=0]\n",
    "y_labels = [english_vectorizer.get_vocabulary()[i] for i in english_teacher_enc[id][:] if i!=0]\n",
    "attention_matrix = attention_matrix[:len(y_labels),:len(x_labels)]\n",
    "print(np.sum(attention_matrix, axis=1))\n",
    "\n",
    "\n",
    "#x_labels.reverse()\n",
    "#y_labels.reverse()\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(3, 3))\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(255, 10, as_cmap=True)\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(attention_matrix, vmax=1.0,\n",
    "            xticklabels=x_labels,\n",
    "            yticklabels=y_labels,\n",
    "            cmap=cmap,\n",
    "            center=0,\n",
    "            square=True,\n",
    "            linewidths=.5,\n",
    "            cbar_kws={\"shrink\": .7},\n",
    "           );\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986f8433",
   "metadata": {},
   "source": [
    "Poids respectifs entre c et h pour prédire lors de la dernière couche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b5e129",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-19T10:14:23.454324Z",
     "start_time": "2022-10-19T10:14:23.333601Z"
    }
   },
   "outputs": [],
   "source": [
    "last_layer = model.layers[-1]\n",
    "weights = last_layer.get_weights()\n",
    "\n",
    "context_weigths = np.sum(weights[0][:64,:], axis=0)\n",
    "hidden_weigths = np.sum(weights[0][64:,:], axis=0)\n",
    "\n",
    "m = np.zeros((len(y_labels), 2))\n",
    "for i, j in enumerate(english_teacher_enc[id][:len(y_labels)]):\n",
    "    # Context from attention // Decoder output\n",
    "    c = abs(context_weigths[j])\n",
    "    h = abs(hidden_weigths[j])\n",
    "    #print(context_weigths[j], hidden_weigths[j])\n",
    "    m[i,:] = [c/(c+h),h/(h+c)]\n",
    "\n",
    "sns.heatmap(m, vmax=1.0,\n",
    "            xticklabels=[\"attention\", \"decoder\"],\n",
    "            yticklabels=y_labels,\n",
    "            cmap=cmap,\n",
    "            center=0,\n",
    "            square=True,\n",
    "            linewidths=.5,\n",
    "            cbar_kws={\"shrink\": .7},\n",
    "           );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6894dfa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-19T09:41:48.026628Z",
     "start_time": "2022-10-19T09:41:48.022314Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in english_teacher_enc[id]:\n",
    "    # Context from attention // Decoder output\n",
    "    c = abs(context_weigths[i])\n",
    "    h = abs(hidden_weigths[i])\n",
    "    #print(context_weigths[i], hidden_wefigths[i])\n",
    "    print(int(100*c/(c+h)), int(100*h/(h+c)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824c5cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Understand the network\n",
    "#Build Attentional network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
