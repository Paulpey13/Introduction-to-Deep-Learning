{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd16617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "626882a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"http://www.i3s.unice.fr/~riveill/dataset/Amazon_Unlocked_Mobile/train.csv.gz\")\n",
    "val = pd.read_csv(\"http://www.i3s.unice.fr/~riveill/dataset/Amazon_Unlocked_Mobile/val.csv.gz\")\n",
    "test = pd.read_csv(\"http://www.i3s.unice.fr/~riveill/dataset/Amazon_Unlocked_Mobile/test.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c64173c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (5000, 6)\n",
      "Test shape :  (1000, 6)\n",
      "Val shape :  (1000, 6)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train shape : \",train.shape)\n",
    "print(\"Test shape : \",test.shape)\n",
    "print(\"Val shape : \",val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fe4235c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 1), (1000, 1))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct X_train and y_train\n",
    "X_train = train['Reviews'].fillna(\"\")\n",
    "X_train = np.array(train['Reviews'].fillna(\"\")).reshape(-1,1)\n",
    "y_train = train['Rating']\n",
    "y_train = np.array(train['Rating']).reshape(-1,1)\n",
    "X_train.shape, y_train.shape\n",
    "\n",
    "# Construct X_test and y_test\n",
    "X_test = test['Reviews'].fillna(\"\")\n",
    "X_test = np.array(test['Reviews']).reshape(-1,1)\n",
    "y_test = test['Rating']\n",
    "y_test = np.array(test['Rating']).reshape(-1,1)\n",
    "X_test.shape, y_test.shape\n",
    "\n",
    "# Construct X_val and y_val\n",
    "X_val = val['Reviews'].fillna(\"\")\n",
    "X_val = np.array(val['Reviews']).reshape(-1,1)\n",
    "y_val = val['Rating']\n",
    "y_val = np.array(val['Rating']).reshape(-1,1)\n",
    "X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7adc7bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paul\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\preprocessing\\_encoders.py:808: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ohe = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "y_train_encoded = ohe.fit_transform(y_train)\n",
    "y_val_encoded = ohe.transform(y_val)\n",
    "y_test_encoded = ohe.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb026d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')\n",
    "stops = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "653bacf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_stops(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_sentence = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stops:\n",
    "            filtered_sentence.append(w)\n",
    "    result=' '.join(filtered_sentence)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd9c47a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the data\n",
    "import unicodedata\n",
    "\n",
    "def step1(sent):\n",
    "    # sent = on sentence in a language\n",
    "    def unicode_to_ascii(s):\n",
    "        return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "    sent = unicode_to_ascii(sent.lower().strip())\n",
    "\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    sent = re.sub(r\"([?.!,¿])\", r\" \\1 \", sent)\n",
    "    sent = re.sub(r'[\" \"]+', \" \", sent)\n",
    "\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    sent = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sent)\n",
    "    \n",
    "    #removing emojis and non latin symbols (chinese, russian etc...)\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    sent= re.sub(emoj, '', sent)\n",
    "    \n",
    "\n",
    "    return '<start> ' + sent.strip() + ' <end>' # and start and stop tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "651a8476",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import re\n",
    "#Those are the cleaning functions we already used in previous machine learning labs\n",
    "\n",
    "def clean_text(text):\n",
    "    text=text.lower() #lowercase the text\n",
    "    #text=re.sub(r'[^\\w\\s]', '', text) #remove punctuation  Voir si besoin de ça\n",
    "    text=del_stops(text) #delete stop words\n",
    "    text=lemmatizer.lemmatize(text)\n",
    "    text=step1(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1020158",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_p = np.array([clean_text(r) for r in X_train.flatten()])\n",
    "X_test_p = np.array([clean_text(r) for r in X_test.flatten()])\n",
    "X_val_p = np.array([clean_text(r) for r in X_val.flatten().astype(str)]) #Add of astype(str) because one of the element was a float and it the clean_text(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05864d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of vocab to keep\n",
    "max_vocab = 20000\n",
    "# length of sequence that will generate\n",
    "max_len = 15\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_vocab)\n",
    "tokenizer.fit_on_texts(X_train_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4da526e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8444"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(X_train_p)\n",
    "word_index = tokenizer.word_index\n",
    "len(word_index) #number of words tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "641aa26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def babysit(history):\n",
    "    keys = [key for key in history.keys() if key[:4] != \"val_\"]\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=len(keys), figsize=(18, 5))\n",
    "    for i, key in enumerate(keys):\n",
    "        ax[i].plot(history[key], label=key)\n",
    "        if \"val_\"+key in history.keys():\n",
    "            ax[i].plot(history[\"val_\"+key], label=\"val_\"+key)\n",
    "        ax[i].legend()\n",
    "        ax[i].set_title(key)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da280d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "embedding_dim = 50\n",
    "latent_dim = 64\n",
    "dropout=0.3\n",
    "epochs=1000\n",
    "patience=5\n",
    "attention_func = 'MLP' # others : 'dot', 'bilinear', 'MLP'\n",
    "max_len=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fa399ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def voc(lang):\n",
    "    # a list of sentences in the same language\n",
    "    lengths = [len(txt.split()) for txt in lang]\n",
    "    vocab = set([w for txt in lang for w in txt.split()])  \n",
    "\n",
    "    return max(lengths), list(vocab), len(vocab)+2 # for padding and OOV\n",
    "max_length_reviews, vocab_reviews, vocab_size_reviews = voc(X_train_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a02969ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['<start> love ! ! ! absolutely love ! ! <end>',\n",
       "       '<start> love blu phones ! second one year anything wrong blu studio . hd . opted upgrade phone slightly smaller screen better battery life . price phones go wrong . . setup breeze . customization easy . colors vibrant . phone light even rugged case . battery life better phone ever had . screen responsive touch unlike phones . i issue volume music playback reviewers noted . great phone little price . everyone give blu try . <end>'],\n",
       "      dtype='<U3643')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_p[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "417479fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rnn_model():\n",
    "    sequences = layers.Input(shape=(100,))\n",
    "    embedded = layers.Embedding(vocab_size_reviews, 64)(sequences)\n",
    "    x = layers.GRU(128, return_sequences=True)(embedded)\n",
    "    x = layers.GRU(128)(x)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    x = layers.Dense(100, activation='relu')(x)\n",
    "    predictions = layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs=sequences, outputs=predictions)\n",
    "    model.compile(\n",
    "        optimizer='rmsprop',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['binary_accuracy']\n",
    "    )\n",
    "    return model\n",
    "    \n",
    "rnn_model = build_rnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06047c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 100)]             0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 100, 64)           540928    \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 100, 128)          74496     \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 128)               99072     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                4128      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               3300      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 722,025\n",
      "Trainable params: 722,025\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "771bd021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# earlyStopping=EarlyStopping(patience=3,monitor='binary_accuracy',min_delta=0.0001,verbose=1)\n",
    "# callbackslist=[earlyStopping]\n",
    "# history = rnn_model.fit(X_train_p,y_train,batch_size=128,epochs=2,verbose=1,callbacks=callbackslist, validation_data=(X_val_p,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f5bb237b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = y_train.shape[1]\n",
    "vocab_size = 10 ** 4  # Maximum vocab size -- adjust with the size of the vocabulary\n",
    "embedding_size = 20  # Embedding size (usually <= 300)\n",
    "recurrent_size = 64  # Recurrent size\n",
    "hidden_size = recurrent_size // 4  # Hidden layer\n",
    "dropout_rate = 0.2  # Dropout rate for regularization (usually between 0.1 and 0.25)\n",
    "max_len = 150  # Sequence length to pad the outputs to (deduced from the length distribution study)\n",
    "learning_rate = 0.0075"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "91b2aced",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Incompatible input shapes: axis values 1 (at axis 1) != 16 (at axis 1). Full input shapes: (None, 1), (None, 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m attention \u001b[38;5;241m=\u001b[39m Dense(\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtanh\u001b[39m\u001b[38;5;124m'\u001b[39m)(state_h)\n\u001b[0;32m     17\u001b[0m attention \u001b[38;5;241m=\u001b[39m Activation(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m)(attention)\n\u001b[1;32m---> 18\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgru\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Concatenation of context vector and GRU output\u001b[39;00m\n\u001b[0;32m     21\u001b[0m merged \u001b[38;5;241m=\u001b[39m Concatenate()([context, state_h])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\layers\\merging\\dot.py:226\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(inputs, axes, normalize, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.layers.dot\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdot\u001b[39m(inputs, axes, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;124;03m\"\"\"Functional interface to the `Dot` layer.\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \n\u001b[0;32m    213\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;124;03m        A tensor, the dot product of the samples from the inputs.\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDot\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\layers\\merging\\dot.py:136\u001b[0m, in \u001b[0;36mDot.build\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    134\u001b[0m     axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shape1[axes[\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m!=\u001b[39m shape2[axes[\u001b[38;5;241m1\u001b[39m]]:\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    137\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible input shapes: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    138\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maxis values \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape1[axes[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (at axis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) != \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    139\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape2[axes[\u001b[38;5;241m1\u001b[39m]]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (at axis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxes[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    140\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull input shapes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    141\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Incompatible input shapes: axis values 1 (at axis 1) != 16 (at axis 1). Full input shapes: (None, 1), (None, 16)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Embedding, GRU, Dense, Concatenate, TimeDistributed, Activation\n",
    "\n",
    "# Input layer\n",
    "inputs = Input(shape=(None,))\n",
    "\n",
    "# Embedding layer\n",
    "embedded = Embedding(input_dim=vocab_size_reviews, output_dim=embedding_dim, input_length=max_len)(inputs)\n",
    "\n",
    "# GRU layer with attention\n",
    "gru, state_h = GRU(units=hidden_size, return_state=True)(embedded)\n",
    "\n",
    "# Attention mechanism\n",
    "attention = Dense(1, activation='tanh')(state_h)\n",
    "attention = Activation('softmax')(attention)\n",
    "context = keras.layers.dot([attention, gru], axes=-1)\n",
    "\n",
    "# Concatenation of context vector and GRU output\n",
    "merged = Concatenate()([context, state_h])\n",
    "\n",
    "# Output layer\n",
    "output = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "# Model definition\n",
    "model = keras.Model(inputs=inputs, outputs=output)\n",
    "\n",
    "# Teacher forcing implementation\n",
    "def generate_sequence(model, tokenizer, max_length, seed_text, n_words):\n",
    "    in_text = seed_text\n",
    "    for _ in range(n_words):\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
    "        yhat = model.predict(encoded, verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                break\n",
    "        in_text += ' ' + word\n",
    "    return in_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b576f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0919a6e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b6b1c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c1fa28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61797a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
