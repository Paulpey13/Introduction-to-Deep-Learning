{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aff76475",
   "metadata": {
    "id": "aff76475"
   },
   "source": [
    "# <p style=\"text-align: center;\"> Home work on Attentional LSTM network </p>\n",
    "### <p style=\"text-align: center;\"> Paul Peyssard </p>\n",
    "### <p style=\"text-align: center;\"> Worked in collaboration with Aglind Reka, Trang Nguyen, Matheus Paula </p>\n",
    "#### <p style=\"text-align: center;\"> MSc Data Science & Artificial Intelligence </p>\n",
    "#### <p style=\"text-align: center;\"> Intro to deep learning </p> \n",
    "#### <p style=\"text-align: center;\"> 11/02/2023 </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dec399",
   "metadata": {
    "id": "29dec399"
   },
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf95295",
   "metadata": {
    "id": "ebf95295"
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wMNYCjcFOzAj",
   "metadata": {
    "id": "wMNYCjcFOzAj"
   },
   "source": [
    "We start by importing the libraries we will need during our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a9600b4",
   "metadata": {
    "id": "0a9600b4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29acdbe9",
   "metadata": {
    "id": "29acdbe9"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cdfa3c2",
   "metadata": {
    "id": "6cdfa3c2"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, r2_score, f1_score,cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8410947",
   "metadata": {
    "id": "f8410947"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e40bc8",
   "metadata": {
    "id": "92e40bc8"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_ruJM7qxO4bS",
   "metadata": {
    "id": "_ruJM7qxO4bS"
   },
   "source": [
    "Now let's download our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9d8636d",
   "metadata": {
    "id": "c9d8636d"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"http://www.i3s.unice.fr/~riveill/dataset/Amazon_Unlocked_Mobile/train.csv.gz\")\n",
    "val = pd.read_csv(\"http://www.i3s.unice.fr/~riveill/dataset/Amazon_Unlocked_Mobile/val.csv.gz\")\n",
    "test = pd.read_csv(\"http://www.i3s.unice.fr/~riveill/dataset/Amazon_Unlocked_Mobile/test.csv.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea8a243",
   "metadata": {
    "id": "9ea8a243"
   },
   "source": [
    "### a) Checking the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rqOuko-xPDXr",
   "metadata": {
    "id": "rqOuko-xPDXr"
   },
   "source": [
    "Let's check the shape of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62deb262",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "62deb262",
    "outputId": "a60e1bc8-5f18-477e-8dde-4e82f371cbb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (5000, 6)\n",
      "Test shape :  (1000, 6)\n",
      "Val shape :  (1000, 6)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train shape : \",train.shape)\n",
    "print(\"Test shape : \",test.shape)\n",
    "print(\"Val shape : \",val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "owtcamvtPJYP",
   "metadata": {
    "id": "owtcamvtPJYP"
   },
   "source": [
    "And let's also check how many classes our target has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5880fe05",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5880fe05",
    "outputId": "bd6557bc-1911-4536-cca5-63e35e089267"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train['Rating'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2Oc1V8VsPeiM",
   "metadata": {
    "id": "2Oc1V8VsPeiM"
   },
   "source": [
    "As we can see, there is a total of five classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NiU1LxNJegdC",
   "metadata": {
    "id": "NiU1LxNJegdC"
   },
   "source": [
    "As can be seen, we have five classes in total, representing each possible rating value (1,2,3,4 or 5)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WMEcKEQ4PbN7",
   "metadata": {
    "id": "WMEcKEQ4PbN7"
   },
   "source": [
    "Let's see how our dataframe looks now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a04e4ce9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "a04e4ce9",
    "outputId": "8313f289-a7e8-4135-94da-e0656cae1417"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Review Votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Samsung Galaxy Note 4 N910C Unlocked Cellphone...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>449.99</td>\n",
       "      <td>4</td>\n",
       "      <td>I love it!!! I absolutely love it!! üëåüëç</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BLU Energy X Plus Smartphone - With 4000 mAh S...</td>\n",
       "      <td>BLU</td>\n",
       "      <td>139.00</td>\n",
       "      <td>5</td>\n",
       "      <td>I love the BLU phones! This is my second one t...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apple iPhone 6 128GB Silver AT&amp;T</td>\n",
       "      <td>Apple</td>\n",
       "      <td>599.95</td>\n",
       "      <td>5</td>\n",
       "      <td>Great phone</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BLU Advance 4.0L Unlocked Smartphone -US GSM -...</td>\n",
       "      <td>BLU</td>\n",
       "      <td>51.99</td>\n",
       "      <td>4</td>\n",
       "      <td>Very happy with the performance. The apps work...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Huawei P8 Lite US Version- 5 Unlocked Android ...</td>\n",
       "      <td>Huawei</td>\n",
       "      <td>198.99</td>\n",
       "      <td>5</td>\n",
       "      <td>Easy to use great price</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Product Name Brand Name   Price  \\\n",
       "0  Samsung Galaxy Note 4 N910C Unlocked Cellphone...    Samsung  449.99   \n",
       "1  BLU Energy X Plus Smartphone - With 4000 mAh S...        BLU  139.00   \n",
       "2                   Apple iPhone 6 128GB Silver AT&T      Apple  599.95   \n",
       "3  BLU Advance 4.0L Unlocked Smartphone -US GSM -...        BLU   51.99   \n",
       "4  Huawei P8 Lite US Version- 5 Unlocked Android ...     Huawei  198.99   \n",
       "\n",
       "   Rating                                            Reviews  Review Votes  \n",
       "0       4             I love it!!! I absolutely love it!! üëåüëç           0.0  \n",
       "1       5  I love the BLU phones! This is my second one t...           4.0  \n",
       "2       5                                        Great phone           1.0  \n",
       "3       4  Very happy with the performance. The apps work...           2.0  \n",
       "4       5                            Easy to use great price           0.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WQUaWWmgPm-s",
   "metadata": {
    "id": "WQUaWWmgPm-s"
   },
   "source": [
    "Let's also check the type of each of our columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abd6e4fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "abd6e4fd",
    "outputId": "c1c84190-83e9-4823-c5b0-a21df63566a6",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Product Name     object\n",
       "Brand Name       object\n",
       "Price           float64\n",
       "Rating            int64\n",
       "Reviews          object\n",
       "Review Votes    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efb4af4",
   "metadata": {
    "id": "4efb4af4"
   },
   "source": [
    "### b) Build X (features vectors) and y (labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473655ba",
   "metadata": {
    "id": "473655ba"
   },
   "source": [
    "For our particular problem, as we did in a previous lab, we will use \"Reviews\" as our explanatory data (X) and Rating as our target (Y). In other words, we want our classifier to predict a rating for a particular review."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frjtxlYHRcM1",
   "metadata": {
    "id": "frjtxlYHRcM1"
   },
   "source": [
    "Let's begin by preprocessing our data. Removing null values and reshaping it so we can vectorize it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e28ac67a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e28ac67a",
    "outputId": "c4125937-3607-440b-daea-70771efb0f62"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 1), (5000, 1))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct X_train and y_train\n",
    "X_train = train['Reviews'].fillna(\"\")\n",
    "X_train = np.array(train['Reviews'].fillna(\"\")).reshape(-1,1)\n",
    "y_train = train['Rating']\n",
    "y_train = np.array(train['Rating']).reshape(-1,1)\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c32e8c9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0c32e8c9",
    "outputId": "de277c83-ce7e-4542-b165-a4d45f6b614a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 1), (1000, 1))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct X_test and y_test\n",
    "X_test = test['Reviews'].fillna(\"\")\n",
    "X_test = np.array(test['Reviews']).reshape(-1,1)\n",
    "y_test = test['Rating']\n",
    "y_test = np.array(test['Rating']).reshape(-1,1)\n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26babd9d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "26babd9d",
    "outputId": "3bd0d7c3-cdbd-48a1-9af7-c06223b2d9fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 1), (1000, 1))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct X_val and y_val\n",
    "X_val = val['Reviews'].fillna(\"\")\n",
    "X_val = np.array(val['Reviews']).reshape(-1,1)\n",
    "y_val = val['Rating']\n",
    "y_val = np.array(val['Rating']).reshape(-1,1)\n",
    "X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3b1068",
   "metadata": {
    "id": "cb3b1068"
   },
   "source": [
    "### c) One-hot-encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6NwW6AwhRqw2",
   "metadata": {
    "id": "6NwW6AwhRqw2"
   },
   "source": [
    "Just like we are going to vectorize our input, we also need to vectorize our target labels so our neural network can learn with it. For this, we can use one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6378da48",
   "metadata": {
    "id": "6378da48"
   },
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "y_train_encoded = ohe.fit_transform(y_train)\n",
    "y_val_encoded = ohe.transform(y_val)\n",
    "y_test_encoded = ohe.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb55f62b",
   "metadata": {
    "id": "eb55f62b"
   },
   "source": [
    "### e) Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b86a13f",
   "metadata": {
    "id": "5b86a13f"
   },
   "source": [
    "We need to do some preprocessing in our text before we can input it to our neural network. For this we can use the Natural Language Toolkit, or \"nltk\": https://www.nltk.org/\n",
    "\n",
    "For this particular purpose we need to download a few packages in order to be able to use it to remove stop words from our text data (the review data). A stop word is a commonly used word that should be ignored during the processing of text data for machine/deep learning purposes. nltk provides us with a library of common stopwords in various languages, including English, so we can remove them seamlessly from our text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d98fc8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "64d98fc8",
    "outputId": "fcab651d-e540-421c-fe02-2a3936acb77f"
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "stops = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee19120",
   "metadata": {
    "id": "7ee19120"
   },
   "source": [
    "We now now write a function ``del_stops`` to delete our stop words from out text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63ff0db",
   "metadata": {
    "id": "a63ff0db"
   },
   "outputs": [],
   "source": [
    "def del_stops(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_sentence = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stops:\n",
    "            filtered_sentence.append(w)\n",
    "    result=' '.join(filtered_sentence)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84c0a3b",
   "metadata": {
    "id": "e84c0a3b"
   },
   "source": [
    "We still need to do some other preprocessing with our text data. For example, it is littered with emojis and non-latin characters. We also need to add tags indicating the start and end of each review. Below, we write a function to clean the data of emojis, non-latin characters and other kinds of special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7c3708",
   "metadata": {
    "id": "0a7c3708"
   },
   "outputs": [],
   "source": [
    "# Process the data\n",
    "import unicodedata\n",
    "\n",
    "def step1(sent):\n",
    "    # sent = on sentence in a language\n",
    "    def unicode_to_ascii(s):\n",
    "        return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "    sent = unicode_to_ascii(sent.lower().strip())\n",
    "\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    sent = re.sub(r\"([?.!,¬ø])\", r\" \\1 \", sent)\n",
    "    sent = re.sub(r'[\" \"]+', \" \", sent)\n",
    "\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    sent = re.sub(r\"[^a-zA-Z?.!,¬ø]+\", \" \", sent)\n",
    "    \n",
    "    #removing emojis and non latin symbols (chinese, russian etc...)\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    sent= re.sub(emoj, '', sent)\n",
    "    \n",
    "\n",
    "    return '<start> ' + sent.strip() + ' <end>' # and start and stop tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1929499b",
   "metadata": {
    "id": "1929499b"
   },
   "source": [
    "We now unite those functions into one single function, ``clean_text``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2b9b63",
   "metadata": {
    "id": "6d2b9b63"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import re\n",
    "#Those are the cleaning functions we already used in previous machine learning labs\n",
    "\n",
    "def clean_text(text):\n",
    "    text=text.lower() #lowercase the text\n",
    "    #text=re.sub(r'[^\\w\\s]', '', text) #remove punctuation  Voir si besoin de √ßa\n",
    "    text=del_stops(text) #delete stop words\n",
    "    text=lemmatizer.lemmatize(text)\n",
    "    text=step1(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0872fcb0",
   "metadata": {
    "id": "0872fcb0"
   },
   "source": [
    "Now let us apply this to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266c1a23",
   "metadata": {
    "id": "266c1a23"
   },
   "outputs": [],
   "source": [
    "X_train_p = np.array([clean_text(r) for r in X_train.flatten()])\n",
    "X_test_p = np.array([clean_text(r) for r in X_test.flatten()])\n",
    "X_val_p = np.array([clean_text(r) for r in X_val.flatten().astype(str)]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mKVRJhDFgqKi",
   "metadata": {
    "id": "mKVRJhDFgqKi"
   },
   "source": [
    "Let's take a look to see if it worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1934dfd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "e1934dfd",
    "outputId": "cfcac8aa-6fb7-42af-def1-831c13385f2f"
   },
   "outputs": [],
   "source": [
    "X_train[0]\n",
    "X_train_p[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1084351",
   "metadata": {
    "id": "a1084351"
   },
   "source": [
    "### Defining the babysit function in order to observe the learning curves of the models\n",
    "\n",
    "This function is very similar to the function we used in previous labs. It basically plots a graph with the evolution of the neural network during its learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d43351",
   "metadata": {
    "id": "57d43351"
   },
   "outputs": [],
   "source": [
    "def babysit(history):\n",
    "    keys = [key for key in history.keys() if key[:4] != \"val_\"]\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=len(keys), figsize=(18, 5))\n",
    "    for i, key in enumerate(keys):\n",
    "        ax[i].plot(history[key], label=key)\n",
    "        if \"val_\"+key in history.keys():\n",
    "            ax[i].plot(history[\"val_\"+key], label=\"val_\"+key)\n",
    "        ax[i].legend()\n",
    "        ax[i].set_title(key)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f591d0",
   "metadata": {
    "id": "e8f591d0"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07188d6c",
   "metadata": {
    "id": "07188d6c"
   },
   "source": [
    "Let's set up a few constants that we will need to build our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934cd937",
   "metadata": {
    "id": "934cd937"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "embedding_dim = 50\n",
    "latent_dim = 64\n",
    "dropout=0.3\n",
    "recurrent_size = 64 \n",
    "nb_classes = 5\n",
    "vocab_size = 10 ** 4  # Maximum vocab size -- adjust with the size of the vocabulary\n",
    "embedding_size = 20  # Embedding size (usually <= 300)\n",
    "recurrent_size = 64  # Recurrent size\n",
    "hidden_size = recurrent_size // 4  # Hidden layer\n",
    "dropout_rate = 0.2  # Dropout rate for regularization (usually between 0.1 and 0.25)\n",
    "max_len = 150  # Sequence length to pad the outputs to (deduced from the length distribution study)\n",
    "learning_rate = 0.0075"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea7dc0b",
   "metadata": {
    "id": "dea7dc0b"
   },
   "source": [
    "Let's take only a small amount of data because it is a very large dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff9ffaf",
   "metadata": {
    "id": "eff9ffaf"
   },
   "outputs": [],
   "source": [
    "# elements=4000\n",
    "# X_train_p=X_train_p[0:elements]\n",
    "# y_train_encoded=y_train_encoded[0:elements]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5666924e",
   "metadata": {
    "id": "5666924e"
   },
   "source": [
    "We now declare a function called ``voc`` which will give us some information about our vocabulary corpus. It basically returns three values: the length of the biggest vectorized vocab array, the vocab array itself as a list and the number of vocabularies itself + 2 (for padding and creating out-of-vocabulary embeddings). We will need this information for our embedding layer later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c85622",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "11c85622",
    "outputId": "1faf2165-011b-4677-fb1e-2322b4de5825"
   },
   "outputs": [],
   "source": [
    "def voc(lang):\n",
    "    lengths = [len(txt.split()) for txt in lang]\n",
    "    vocab = set([w for txt in lang for w in txt.split()])  \n",
    "    return max(lengths), list(vocab), len(vocab)+2 # for padding and OOV\n",
    "\n",
    "max_length_reviews, vocab_reviews, vocab_size_reviews = voc(X_train_p)\n",
    "rate_size=5\n",
    "print(max_length_reviews, vocab_size_reviews,rate_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b67b50f",
   "metadata": {
    "id": "7b67b50f"
   },
   "source": [
    "We now build a vectorizer for our reviews, which will transform our strings into a numerical vectorized representation and build a vocabulary linked to said representation while at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d25cc6",
   "metadata": {
    "id": "f3d25cc6"
   },
   "outputs": [],
   "source": [
    "reviews_vectorizer = layers.TextVectorization(\n",
    "    max_tokens=vocab_size_reviews,\n",
    "    standardize=\"lower_and_strip_punctuation\",\n",
    "    # it is possible to build your own function\n",
    "    # to transform emoji into text\n",
    "    # to transform foreign reviews in english one\n",
    "    # etc.\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length_reviews,\n",
    ")\n",
    "\n",
    "reviews_vectorizer.adapt(X_train_p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd6d8a0",
   "metadata": {
    "id": "8cd6d8a0"
   },
   "source": [
    "### Let's build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7465e7",
   "metadata": {
    "id": "5e7465e7"
   },
   "source": [
    "We start by building our input layer, which will take an one-dimensional list of reviews - represented as strings - as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a05f5c8",
   "metadata": {
    "id": "1a05f5c8"
   },
   "outputs": [],
   "source": [
    "inputLayer = layers.Input(shape=(1,), name=\"input\", dtype=tf.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3b3acb",
   "metadata": {
    "id": "4c3b3acb"
   },
   "source": [
    "As we know, we need to vectorize our text data - transform it into an array of numbers linked to a vocabulary - before we can learn with it as neural networks cannot learn with text data itself. So we vectorize our text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ba801f",
   "metadata": {
    "id": "62ba801f"
   },
   "outputs": [],
   "source": [
    "vect = reviews_vectorizer(inputLayer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66850c1a",
   "metadata": {
    "id": "66850c1a"
   },
   "source": [
    "Then we add an embedding layer so our neural network can learn the correct word embeddings. This lets the neural network transform our text data into vectorized data, and lets us be sure that our sequences are of the same length (``embedding_size``)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac30b98",
   "metadata": {
    "id": "5ac30b98"
   },
   "outputs": [],
   "source": [
    "embedding = layers.Embedding(\n",
    "    input_dim=vocab_size,\n",
    "    output_dim=embedding_size,\n",
    "    weights=None,  # Without pre-learning\n",
    "    trainable=True,  # Trainable\n",
    "    name=\"embedding\",\n",
    ")(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4118d326",
   "metadata": {
    "id": "4118d326"
   },
   "source": [
    "Now we add a layer with recurrent (GRU) cells. The GRU cell processes the sequences of input data by selectively updating and resetting its hidden state at each time step, allowing it to capture long-term dependencies in the input sequence. ``recurrent_size`` is the number of hidden units in the layer, while ``return_sequences``, if set to True, will make the neural network return an output for each time step in the sequence, and ``return_state`` does the same but only for the state. In this case we just want the sequences so we set ``return_sequences`` to ``True`` and ``return_state`` to ``False``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af4afbe",
   "metadata": {
    "id": "1af4afbe"
   },
   "outputs": [],
   "source": [
    "rnn = layers.GRU(\n",
    "    recurrent_size,\n",
    "    return_sequences=True,\n",
    "    return_state=False,\n",
    "    dropout=dropout_rate,\n",
    "    recurrent_dropout=dropout_rate,\n",
    ")(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "T4UMARqhqt_8",
   "metadata": {
    "id": "T4UMARqhqt_8"
   },
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b205bb",
   "metadata": {
    "id": "d2b205bb"
   },
   "source": [
    "The last output of our GRU layer is a context vector of fixed size. A critical disadvantage with working with a context vector of fixed size is that the network becomes incapable of remembering the large sentences. We can often face problems like forgetting the starting part of the sequence after processing the whole sequence, for example. To solve this issue, we can use an attention mechanism.\n",
    "\n",
    "A attention mechanism is a mechanism that can help a neural network to memorize long sequences of the information or data. It does so by creating a shortcut between the entire input and the context vector where the weights of the shortcut connection can be changeable for every output. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4955cdfd",
   "metadata": {
    "id": "4955cdfd"
   },
   "source": [
    "For our structure, we will create a Dense layer with one unit and which uses ``tanh`` as the activation function to serve as the attention layers. It will map the output of GRU to an attention score (a single one) for each time step in the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126b0cca",
   "metadata": {
    "id": "126b0cca"
   },
   "outputs": [],
   "source": [
    "attention = layers.Dense(1, activation=\"tanh\")(rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a330b7",
   "metadata": {
    "id": "63a330b7"
   },
   "source": [
    "This line flattens the output of the previous Dense layer to a 1 dimension tensor. This gives us the probability distribution over the time steps of the GRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55269fba",
   "metadata": {
    "id": "55269fba"
   },
   "outputs": [],
   "source": [
    "attention = layers.Flatten()(attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee11746",
   "metadata": {
    "id": "8ee11746"
   },
   "source": [
    "Although we did not see Repeat Vector in class, I decided to introduce the idea in this assignment as our structure is not exactly the same we saw in class.\n",
    "\n",
    "The Repeat Vector is used to create a tensor with the shape ``(batch_size,timesteps,recurrent)`` by repeating the attention scores tensor along the time step dimension.\n",
    "\n",
    "There is more information about it in this article: https://towardsdatascience.com/gru-recurrent-neural-networks-a-smart-way-to-predict-sequences-in-python-80864e4fe9f6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae852300",
   "metadata": {
    "id": "ae852300"
   },
   "outputs": [],
   "source": [
    "attention = layers.RepeatVector(recurrent_size)(attention)  # NORMAL RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a02810",
   "metadata": {
    "id": "14a02810"
   },
   "source": [
    "We now create a \"Permute\" layer, which is used to transpose the dimension of the tensor. It is necessary to ensure that the attention score are correctly aligned with the ouput of GRU layer when attention is applied to its output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4eab40",
   "metadata": {
    "id": "7f4eab40"
   },
   "outputs": [],
   "source": [
    "attention = layers.Permute([2, 1])(attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191279d6",
   "metadata": {
    "id": "191279d6"
   },
   "source": [
    "Compute the dot product of the layers (GRU and attention), so we can have our final weighted sentence. The importance of each time step is determined by the corresponding attention score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2766731e",
   "metadata": {
    "id": "2766731e"
   },
   "outputs": [],
   "source": [
    "sent_representation = layers.Dot(axes=1, normalize=False)([rnn, attention])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dad0eea",
   "metadata": {
    "id": "1dad0eea"
   },
   "source": [
    "Flatten the output of the dot product so we can transport it to our next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2048bb",
   "metadata": {
    "id": "fb2048bb"
   },
   "outputs": [],
   "source": [
    "flatten = layers.Flatten()(sent_representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3a7a2f",
   "metadata": {
    "id": "6b3a7a2f"
   },
   "source": [
    "Then apply dropout to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac502eb3",
   "metadata": {
    "id": "ac502eb3"
   },
   "outputs": [],
   "source": [
    "hidden_dense = layers.Dropout(dropout_rate)(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6774085",
   "metadata": {
    "id": "f6774085"
   },
   "source": [
    "Output layer of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4326827e",
   "metadata": {
    "id": "4326827e"
   },
   "outputs": [],
   "source": [
    "outputLayer = layers.Dense(nb_classes, activation=\"softmax\")(hidden_dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367fab05",
   "metadata": {
    "id": "367fab05"
   },
   "source": [
    "Create the model with its input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967fba61",
   "metadata": {
    "id": "967fba61"
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=inputLayer, outputs=outputLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a5301f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c8a5301f",
    "outputId": "6144f9c9-7499-426d-db46-69f1973d5ceb"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61198a3",
   "metadata": {
    "id": "d61198a3"
   },
   "source": [
    "Let's setup early stopping now, so the neural network stops learning when accuracy stops improving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622e0310",
   "metadata": {
    "id": "622e0310"
   },
   "outputs": [],
   "source": [
    "callbacks_list = [EarlyStopping(monitor='val_accuracy', mode='max',\n",
    "                                patience=5, restore_best_weights=True, verbose=1,\n",
    "                                )]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c7be93",
   "metadata": {
    "id": "14c7be93"
   },
   "source": [
    "Let us create and Adam optimizer to use when compiling our model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b5881e",
   "metadata": {
    "id": "99b5881e"
   },
   "outputs": [],
   "source": [
    "op = Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab048de4",
   "metadata": {
    "id": "ab048de4"
   },
   "source": [
    "And now, let's compile it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78145f3a",
   "metadata": {
    "id": "78145f3a"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=op, loss=\"categorical_crossentropy\", metrics='accuracy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9b403d",
   "metadata": {
    "id": "7b9b403d"
   },
   "source": [
    "Finally, let us fit the model to your data and make it learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c540c48b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c540c48b",
    "outputId": "066504c5-7248-44b8-dfd6-b560615764b1"
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train_p,y_train_encoded,\n",
    "                    validation_data=(X_val_p, y_val_encoded),\n",
    "                    epochs=4000, batch_size=BATCH_SIZE,\n",
    "                    callbacks=callbacks_list,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbafb639",
   "metadata": {
    "id": "fbafb639"
   },
   "source": [
    "Let us check out the evolution of our neural network in terms of loss and accuracy, for both training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c66dd10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "id": "4c66dd10",
    "outputId": "fccc7ded-e6b1-4ff7-c631-d2fd32522f95"
   },
   "outputs": [],
   "source": [
    "babysit(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd0051b",
   "metadata": {
    "id": "7bd0051b"
   },
   "source": [
    "Now let us predict and plot our confusion matrix to see how well our neural network is working in terms of prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33184971",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "id": "33184971",
    "outputId": "d18da077-08f7-4957-fbd8-825de7252817"
   },
   "outputs": [],
   "source": [
    "y_pred=model.predict(X_train_p)\n",
    "ConfusionMatrixDisplay.from_predictions(y_train_encoded.argmax(1), y_pred.argmax(1), cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IaQ8l9i89kIa",
   "metadata": {
    "id": "IaQ8l9i89kIa"
   },
   "source": [
    "As we can see, our neural network has fairly decent accuracy, reaching about 65% accuracy in total, and exhibiting a confusion matrix with very few mistakes per class, being able to properly predict even the non-extreme ratings fairly well (2/3/4). It is also learning, as we can see from the trend of loss decrease and accuracy increase. However, it does suffer quite a bit with early overfitting even after using dropout, as unfortunate as it is."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
